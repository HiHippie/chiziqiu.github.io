<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>graphSAGE阅读笔记</title>
    <url>/2020/03/16/graphSAGE%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="GraphSAGE《Inductive-Representation-Learning-on-Large-Graphs》阅读笔记"><a href="#GraphSAGE《Inductive-Representation-Learning-on-Large-Graphs》阅读笔记" class="headerlink" title="GraphSAGE《Inductive Representation Learning on Large Graphs》阅读笔记"></a>GraphSAGE《Inductive Representation Learning on Large Graphs》阅读笔记</h1><p><strong>Task</strong>：node classification</p>
<p>最近在读GNN的经典文章，网上对这些文章的解读已经非常透彻，本人在阅读文献过程中也学习了各路大神的独到见解，感谢，向你们致敬。在此分享我的阅读心得和热爱。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>GraphSAGE</strong>为解决大多算法在embeddings训练时需要所有nodes参与的问题（对大图不友好）而生。文章提到现有方法是<strong>直推式transductive</strong>的，不能泛化到<strong>未知节点unseen nodes</strong>。GraphSAGE作为一种<strong>归纳式inductive</strong>框架，不仅能够泛化unseen node，还能够通过从node的<strong>局部邻节点</strong>中通过<strong>采样和聚合</strong>学习embeddings，而不是独立训练每个node的embeddings：</p>
<p>“Here we present GraphSAGE, a general inductive framework that leverages node feature information to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node’s local neighborhood”</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在实际应用场景下需要对未知节点或者整个新图快速的生成嵌入。之前的一些直推式的node embedding方法局限在固定的图（fixed graph），而很多图是不断演变的，这就导致了这些直推方法在新的节点或图出现时要重新训练，效率很低。所以能够适应不断演变的图和节点的归纳能力变得至关重要。作者还提到这种归纳能力应该可以在具有相同特征形式的不同图之间泛化。所谓<strong>transductive</strong>，即节点分类任务中，GCN在训练时是<strong>同时需要训练节点和测试节点</strong>的，但在实际应用中测试节点是不断出现的，所以直推式是不够友好的。而对于大多数机器学习方法或问题，都是<strong>inductive</strong>的，因为训练样本/验证样本/测试样本往往是独立的，显然这种inductive方式更恰当。</p>
<p><strong>inductive</strong>方法难点在于：<strong>边训练边归纳</strong>。</p>
<p>虽然GraphSAGE旨在学习归纳式的节点嵌入方法，但是基于采样的方法使得他们能够掌握图的<strong>结构信息</strong>，用原文的话来讲就是：</p>
<p>“reveal both the node’s local role in the graph, as well as its global position.”</p>
<p>所以，即使是<strong>没有节点特征的图</strong>，GraphSAGE也是适用的。</p>
<p>GraphSAGE通过一组<strong>聚合函数(Aggregator functions)</strong>从节点的<strong>局部邻节点(local neighborhood)</strong>中聚合特征信息。如下图所示：</p>
<img src="/images/GrapeSAGE aggregator.png" style="zoom:50%;" />

<p>大致流程是：每个聚合函数以<strong>不同数量</strong>的<strong>跳数(hops)</strong>或者<strong>搜索深度(search depth)</strong>聚合信息。在test时候，直接应用训练好的聚合函数去处理unseen node。</p>
<p>对于损失函数，GraphSAGE使用了<strong>无监督</strong>方式，而不是任务驱动监督(task-specific supervision)的方式，当然监督的方式也是OK的。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Embedding-generation-forward-propagation"><a href="#Embedding-generation-forward-propagation" class="headerlink" title="Embedding generation(forward propagation)"></a>Embedding generation(forward propagation)</h3><p>首先定义k个聚合函数：<br>$$<br>AGGREGAT{E_k},{\rm{ }}\forall k \in { 1, \ldots ,K}<br>$$<br>以及用于在模型不同层或搜索深度“Search Depth”之间传播信息的权重矩阵：<br>$$<br>{W^k},{\rm{ }}\forall k \in { 1, \ldots ,K}<br>$$<br>算法流程图：</p>
<img src="/images\graphSAGE workflow.png" style="zoom:40%;" />

<p>算法流程所展示的直觉是：方法在每一次迭代过程(search depth)中，节点都会从邻节点中聚合信息。随着迭代次数增多，节点<strong>增量地</strong>获取越来越多的关于图的信息。</p>
<p><strong>K代表着K个聚合函数、K个权重矩阵、以及K层（可以理解为访问到的邻节点的hops）。</strong></p>
<p><strong>1. 采样：</strong>作者在每一次外层循环中，通过采样只选择部分样本进去内层循环。</p>
<p>“instead of iterating over all nodes, we compute <strong>only the representations that are necessary to</strong><br><strong>satisfy the recursion at each depth</strong>”</p>
<p>外层节点样本从<strong>固定数量邻节点</strong>中均匀采样得到，而不是所有邻节点。</p>
<p><strong>2. 聚合：</strong>每个node将从邻节点(Immediate Neighborhood)中得到的信息聚合为一个vector:</p>
<img src="/images\sage聚合公式.png" style="zoom:60%;" />

<p>后将聚合得到的vector和该节点自身的vector <strong>concatenate</strong>到一起，接上一个全连接层，得到的vector作为下一步的输入，记作z_v。至于如何聚合是比较灵活的，作者在后文也有讨论。</p>
<h3 id="Learning-the-parameters-of-GraphSAGE"><a href="#Learning-the-parameters-of-GraphSAGE" class="headerlink" title="Learning the parameters of GraphSAGE"></a>Learning the parameters of GraphSAGE</h3><p>如前文所述，作者设计了无监督损失来优化聚合函数的参数和权重W。</p>
<img src="/images\sage目标函数.png" style="zoom:60%;" />

<p>其中<strong>第一项</strong>使得z_u与其“随机游走Random Walk”邻节点z_v<strong>更相似</strong>。</p>
<p><strong>第二项</strong>使得z_u与其非邻居节点“negative samples”<strong>差异化</strong>。</p>
<p>上述的相似性通过向量点积计算得到，<em>P</em>，<em>Q</em>分别表示negative samples的分布和数量。</p>
<p>另外除了无监督损失，还可以根据特定任务设计task-specific objective.</p>
<h3 id="Three-Aggregator-Architectures"><a href="#Three-Aggregator-Architectures" class="headerlink" title="Three Aggregator Architectures"></a>Three Aggregator Architectures</h3><h4 id="Mean-Aggregator"><a href="#Mean-Aggregator" class="headerlink" title="Mean Aggregator"></a>Mean Aggregator</h4><p>第一种方式就是elementwise mean，就是将深度搜索到的邻居节点在每一个维度上取平均值。<br>$$<br>h_v^k \leftarrow \sigma (W \cdot (MEAN({ h_v^{k - 1}})\cup { h_u^{k - 1},\forall u \in {\cal N}(v)}) )<br>$$</p>
<h4 id="LSTM-Aggregator"><a href="#LSTM-Aggregator" class="headerlink" title="LSTM Aggregator"></a>LSTM Aggregator</h4><p>这个就是把LSTM用作聚合函数了，但是它本身并不是对称的，也就是并不能保证“<strong>排列不变量</strong>”。所以这里就先随机排列节点的embeddings，然后再扔进LSTM。</p>
<h4 id="Pooling-Aggregator"><a href="#Pooling-Aggregator" class="headerlink" title="Pooling Aggregator"></a>Pooling Aggregator</h4><p>简单来说就是<strong>全连接层+elementwise max-pooling</strong>，具体如下：</p>
<img src="/images\sage pool.png" style="zoom:60%;" />

<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文的核心在于<strong>伪代码的4、5行（聚合）</strong>和关于<strong>sampling</strong>的设计。一方面GraphSAGE的采样方法对实际应用场景中的大图和可变图很友好；另一方面其聚合思想也使得节点特征的学习不再是独立的，而是更多的融合了邻节点的信息。</p>
<p>另外作者也谈到了GraphSAGE与GCN的联系。GCN是直推式的和全局的，每次迭代都需要整张图的邻接矩阵（包括测试节点），GraphSAGE算是对GCN的一种精简。</p>
<p>但是对于固定数量的采样可能有待商榷，我的理解是每个节点对于整张图的贡献度可能是不同的，那么全部去sampling同样的hops可能不够完善，所以GATs大概能解决这个问题吧。当然了，固定数量是为了能够更方便的做concatenation。</p>
<p>个人理解，如有疏漏欢迎批评指正。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://mp.weixin.qq.com/s/VJn-Sek_aP02MOdcrzbtQA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/VJn-Sek_aP02MOdcrzbtQA</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/62750137" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62750137</a></p>
<p>欢迎关注，一起来玩：</p>
<img src="\images\公众号2.0.jpg" style="zoom:50%;" />]]></content>
      <tags>
        <tag>GNNs</tag>
      </tags>
  </entry>
  <entry>
    <title>图分类《Hierarchical Graph Representation Learning with Differentiable Pooling》阅读笔记</title>
    <url>/2020/03/11/hierarchical/</url>
    <content><![CDATA[<h1 id="图分类《Hierarchical-Graph-Representation-Learning-with-Differentiable-Pooling》阅读笔记"><a href="#图分类《Hierarchical-Graph-Representation-Learning-with-Differentiable-Pooling》阅读笔记" class="headerlink" title="图分类《Hierarchical Graph Representation Learning with Differentiable Pooling》阅读笔记"></a>图分类《Hierarchical Graph Representation Learning with Differentiable Pooling》阅读笔记</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>对于graph neural networks (GNNs)，大体上分三种任务：<strong>Node Classification</strong>，<strong>Link Prediction</strong>和 <strong>Graph Classification</strong>。本文关注的是<strong>Graph Classification</strong>。作者提到，现有的GNN本质上是<strong><em>flat</em></strong>的，而没有学习到<strong><em>hierarchical</em></strong>的图表达。原文描述如下：</p>
<p>‘’However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs—a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph.‘’</p>
<p>为此，作者提出了<strong><em>DIFFPOOL</em></strong>，一种能够学习到图的层次表达并可以<strong>结合多种端到端的GNNs结构</strong>可微的图池化模块。<strong><em>DIFFPOOL</em></strong>为每一层的<em>nodes<em>学习可微的软聚类(**</em>soft*** *cluster</em> <em>assignment</em>)， 将<em>nodes<em>映射到一组簇中，这一组簇将作为下一层</em>GNN Layer<em>的输入(</em>coarsened input</em>)。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><p>对于<strong><em>flat</em></strong>的不足和<strong><em>hierarchical</em></strong>的必要性，作者描述为：<strong><em>flat</em></strong>的本质是聚合信息只能通过<em>edges</em>完成而不能以<strong><em>hierarchical</em></strong>的方式推断和聚合信息。原文描述如下：</p>
<p>“A major limitation of current GNN architectures is that they are inherently flat as they<br>only propagate information across the edges of the graph and are unable to infer and aggregate the information in a hierarchical way.”</p>
<p>标准的方法首先为所有<em>nodes</em>学习<em>embeddings</em>，然后通过<em>globally pool</em>聚合所有的<em>node embeddings</em>。比如用简单的<em>simple summation</em>或者作用在整个集合上的神经网络。作者列举了四篇采用此类标准方法的文章：</p>
<p>[1] <a href="http://proceedings.mlr.press/v48/daib16.pdf" target="_blank" rel="noopener">http://proceedings.mlr.press/v48/daib16.pdf</a></p>
<p>[2] <a href="https://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf</a></p>
<p>[3] <a href="https://arxiv.org/pdf/1704.01212.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.01212.pdf</a></p>
<p>[4] <a href="https://arxiv.org/pdf/1511.05493.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1511.05493.pdf</a></p>
<p>关于此类标准方法的<em>limitations</em>，作者还是强调<em>hierarchical</em>结构更适用于<em>Graph Classification</em>任务。</p>
<p>作者所提方法类似于<em>CNNs<em>中的空间池化(</em>Spatial Pooling Operation</em>)。难点在于，<em>gragh<em>结构没有空间局部性(</em>notion of spatial locality</em>)，也就是说难以直接地将<em>nodes<em>池化到一起，因为</em>graph<em>的拓扑结构不能直接构建”</em>patch</em>“。并且不同的<em>graphs</em>中<em>nodes</em>和<em>edges</em>的数量也不同。</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>聚类<em>nodes<em>来构建一个在低层图(</em>underlying graph</em>)上面的<strong>分层次的多层结构(<em>hierarchical multi-layer scaffold</em>)</strong>。<em>DIFFPOOL<em>在</em>GNN<em>每一层学习一个**</em>soft assignment<strong><em>，使</em>nodes*映射为</strong>簇集合<strong>。然后通过*</strong>stacking*** <em>GNN layers</em>构建分层结构(*hierarchical fashion</em>)。</p>
<p><img src="/images/framework.png" alt="framework"></p>
<p>上图所展示的对<em>DIFFPOOL</em>的<em>High-level</em>的说明：在每一个<em>hierarchical layer</em>，首先用一个<em>GNN Layer</em>学习<em>nodes</em>的<em>embeddings</em>，然后用学习到的<em>embeddings</em>对<em>nodes</em>进行<em>cluster</em>得到<em>coarsened graph</em>，接着用另一个<em>GNN Layer</em>学习<em>coarsened graph</em>。</p>
<h2 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h2><p>首先是<em>Graph</em>的<em>Notation</em>：</p>
<img src="/images/Notation.png" alt="notation" style="zoom:67%;" />

<p><em>GNNs<em>的”</em>Message-Passing</em>“结构：</p>
<img src="/images/messagepassing.png" alt="messagepassing" style="zoom:67%;" />

<p><em>H</em>为<em>node embeddings</em>,即“<em>Messages</em>”。<em>M</em>为<em>message propagation</em>函数。<strong><em>DIFFPOOL</em>方法对<em>M</em>不敏感</strong>，也就是适用于各种<em>M</em>。</p>
<p>给定<em>GNN Layer<em>的输出</em>Z = GNN(A,X)<em>，邻接矩阵</em>A<em>。该方法要学习一个新的</em>coarsened graph (m &lt; n)<em>，</em>A’  (nxn)</em>, <em>Z’ (mxd)</em>。然后<em>coarsened graph</em>作为其他<em>GNN Layer</em>的输入。迭代K次，如上面的框架图所示。现在要解决的问题是，如果利用<em>Z</em>来<em>cluster</em>或<em>pool</em>本层的<em>nodes</em>，从而能够让<em>coarsened graph</em>作为下一层的输入，而且这种方法要适用于各种<em>M</em>和每个<em>Layer</em>。</p>
<h4 id="如何获取coarsened-graph"><a href="#如何获取coarsened-graph" class="headerlink" title="如何获取coarsened graph"></a>如何获取<em>coarsened graph</em></h4><p>定义第<em>l</em>层的<strong><em>Cluster Assignment Matrix</em></strong>：</p>
<img src="/images/cluster assignment matrix.png" alt="cluster assignment matrix" style="zoom:67%;" />

<p><strong><em>S</em></strong>每行表示<em>l</em>层的一个<em>node(cluster)</em>，每一列表示<em>l+1</em>层的一个<em>node(cluster)</em>。作者称之为一种<strong><em>soft assignment</em></strong>。这里看代码比较好理解。</p>
<p>在奥格找法师开了个传送门：<a href="https://github.com/EstelleHuang666/gnn_hierarchical_pooling" target="_blank" rel="noopener">https://github.com/EstelleHuang666/gnn_hierarchical_pooling</a></p>
<p><em>DIFFPOOL</em>定义如下：</p>
<img src="/images/DIFFPOOL1.png" alt="DIFFPOOL1" style="zoom:67%;" />

<img src="/images/DIFFPOOL2.png" alt="DIFFPOOL2" style="zoom:67%;" />

<p>A为实矩阵，表示全连接的加权图(<em>a fully connected edge-weighted graph</em>)。<em>A_ij<em>表示</em>cluster i<em>与</em>cluster j<em>的连接强度(</em>Connectivity Strength</em>)。<em>X</em>中的第<em>i</em>行表示<em>cluster</em> <em>i</em>的<em>embedding<em>。**</em>Coarsened adjacency matrix</em> <em>A**</em>和<strong><em>Cluster embeddings X</em></strong>将作为另一个<em>GNN Layer</em>的输入。</p>
<h4 id="如何获取assignment-matrix-S和embedding-matrices-Z"><a href="#如何获取assignment-matrix-S和embedding-matrices-Z" class="headerlink" title="如何获取assignment matrix S和embedding matrices Z"></a>如何获取<em>assignment matrix</em> <em>S</em>和<em>embedding matrices</em> <em>Z</em></h4><p>作者通过同样以<strong><em>cluster node features X</em></strong> 和 <strong><em>coarsened adjacency matrix A</em></strong>为输入的两个<strong>独立</strong>的GNN Layer分别生成<strong><em>S</em></strong>和<strong><em>Z</em></strong>。生成<em>Z</em>的<em>GNN</em>就是标准的<em>GNN</em>模块，命名为<strong><em>embedding GNN</em></strong>：</p>
<img src="/images/Z.png" alt="Z" style="zoom:67%;" />

<p>生成<em>S</em>的<em>GNN</em>为加了<em>softmax</em>的<em>GNN Layer</em>：</p>
<img src="/images/S.png" alt="softmax" style="zoom:67%;" />

<p>这里的<em>softmax</em>是<strong><em>row-wise fashion</em></strong>的。从矩阵乘法的角度去看这个公式：</p>
<img src="/images/lijie.png" alt="lijie" style="zoom:67%;" />

<p>相当于<em>(l+1, l) x (l, d) = (l+1, d)</em>，<strong>也就是基于多个权重的对nodes在每一维特征上的加权聚合</strong>。这里的多个权重，我理解为作者所提到的<strong><em>hierarchical</em></strong>。关于每一层中<em>cluster</em>数量的设置为超参，最后<em>cluster</em>数量为1用于获取最终的<strong><em>graph embedding vector</em></strong>。</p>
<h4 id="关于优化"><a href="#关于优化" class="headerlink" title="关于优化"></a>关于优化</h4><p>关于优化，是用梯度下降法完成的。但作者提到在训练过程中对下式的优化困难。</p>
<img src="/images/youhua.png" alt="youhua" style="zoom:67%;" />

<p>所以这里应用了<strong><em>auxiliary link prediction objective</em></strong>，使临界节点能够被<em>pool</em>在一起。在每一层，作者通过下式优化：</p>
<img src="/images/objective.png" alt="objective" style="zoom:67%;" />

<p>作者提到的关于<em>Pooling GNN</em>另一个重要的性质是每个node的cluster assignment的输出应该倾向于one-hot的形式，从而使cluster之间的关系更加清晰。所以作者提出通过优化下式来达到上述目的：</p>
<img src="/images/L.png" alt="L" style="zoom:67%;" />

<p>最后每层的L_LP和L_E和分类损失相加。作者提到加了这两个损失函数会使收敛变慢，但效果更好。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>精巧的艺术品，向优秀的作者致敬</p>
<p>除了复杂度略大</p>
<p>才疏学浅，欢迎批评</p>
<p>慢慢来比较快</p>
<p>有个公众号是我和好朋友一起玩的，欢迎来一起交流。对，没头像。</p>
<img src="/images/gzh.jpg" alt="gzh" style="zoom:33%;" />

]]></content>
      <categories>
        <category>GNNs</category>
      </categories>
      <tags>
        <tag>GNNs</tag>
      </tags>
  </entry>
  <entry>
    <title>大道至简----多示例学习与注意力机制的巧妙结合</title>
    <url>/2020/03/02/%E5%A4%A7%E9%81%93%E8%87%B3%E7%AE%80-%E5%A4%9A%E7%A4%BA%E4%BE%8B%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%B7%A7%E5%A6%99%E7%BB%93%E5%90%88/</url>
    <content><![CDATA[<h1 id="大道至简—-多示例学习与注意力机制的巧妙结合"><a href="#大道至简—-多示例学习与注意力机制的巧妙结合" class="headerlink" title="大道至简—-多示例学习与注意力机制的巧妙结合"></a>大道至简—-多示例学习与注意力机制的巧妙结合</h1><p>谈谈《Attention-based deep multiple instance learning》 ICML 2018</p>
<hr>
<p>分享一篇十分简约且对我至关重要的一篇优秀的会议论文《Attention-based deep multiple instance learning》。首先分别谈谈我对多示例学习和注意力机制的理解，再聊一下这篇文章的精彩之处。</p>
<h2 id="01-多示例学习"><a href="#01-多示例学习" class="headerlink" title="01 多示例学习"></a><u><strong>01</strong></u> 多示例学习</h2><p>谈到多示例学习就一定要向南京大学周志华老师致敬，周老师是多示例学习领域的领航员。在此引用周老师对多示例学习的描述[1]：</p>
<p>“在多示例学习中，训练样本是由多个示例组成的包，包是有概念标记的，但示例本身却没有概念标记。如果一个包中至少包含一个正例，则该包是一个正包，否则即为反包。学习的目的是预测新包的类别。”</p>
<p>我们以直观方式表述了多示例的概念(Fig. 1)，图中内容非常直观就不做描述了。</p>
<p><img src="/images/1.png" alt="j"></p>
<p>Fig. 1. Illustration of a MIL Problem (引用自[2])</p>
<p>多示例学习中的关键是找到示例与包之间的逻辑关系，因为示例本身是无标签的而其所属的包是有标签的，所以这是一种弱监督学习框架下的特殊范式。公式(1-3)是应用较多的三种方式，看起来非常的简单但却是近年的主要方法。公式(1)所应用的逻辑在多示例领域被称为示例平均池化，即首先对示例得分(概率空间)进行判别，而后取所有示例得分平均值作为包的结果。公式(2)所展示的方式称为最大池化，即选择<em>Key Instance</em>，也就是找出得分最高的关键示例来代表其包的结果。而第三种所要讨论的，就是基于注意力机制的方式。</p>
<p><img src="/images/2.png" alt="formula"></p>
<h2 id="02-注意力机制"><a href="#02-注意力机制" class="headerlink" title="02 注意力机制"></a><u><strong>02</strong></u> 注意力机制</h2><p>注意力机制（Attention Mechanism）是解决信息超载问题的主要手段的一种资源分配方案，将计算资源分配给更重要的任务。通俗且不是很严谨的说，就是一种被赋予了直观意义的权重，它决定着哪一部分信息更加重要。与上文结合来说，公式(1)和公式(2)可以认为是多示例问题中的一种硬性注意力。</p>
<p>公式(1)中每个示例的注意力权重可以认为是：1/n</p>
<p>公式(2)中<em>Key Instance</em>权重为1.0，而其余示例的注意力权重均为0。</p>
<h2 id="03-示例注意力"><a href="#03-示例注意力" class="headerlink" title="03 示例注意力"></a><u><strong>03</strong></u> 示例注意力</h2><p><img src="/images/mi-net.png" alt="formula"></p>
<p>Fig. 2. The Framework of MI-Net (引用自[3])</p>
<p>Fig. 2 是经典的多示例网络，没错就是如此简单和经典。他与我们所熟悉的全连接网络区别在于，网络末端增加了符合多示例假设的模块：Instance Score Layer和MIL Pooling Layer。上述经典的网络结构中所使用的Pooling方法正是上文提到的最大池化。</p>
<p>那么《Attention-based deep multiple instance learning》文章所采用的注意力机制是如何实现的呢？我们首先定义包<em>H</em>，<em>h_i</em>表示H中的示例。那么对包H的加权和就如公式(7)所示。动态的权重a则借鉴了加性注意力模型思想进行构建(公式8)，其中<em>W</em>和<em>V</em>为网络参数。</p>
<p><img src="/images/4.png" alt="j"></p>
<p>另外，该文章作者提到：tanh激活函数在表达非线性复杂关系是效果不好。因为tanh值域在[-1,+1]之间，在一定程度上限制了示例之间关系的表达。所以作者进一步引入了门控思想构建了Gated Attention。从公式(9)中也不难看出，实际上就是多了一步激活函数为Sigmoid的加权，并以element-wise形式结合。其实这一部分作者并没有给出充分的理论依据，我想此处大多数的idea来源于经验。如果讲的理论些，我更愿意理解为是对两种激活函数不同特性的集成。而且从该论文实验中也看得出，Gated的方案也并不是永远优于第一个版本的。</p>
<p><img src="/images/5.png" alt="formula"></p>
<p>该文章所设计的实验是我最喜欢的。对于相对小众的多示例领域，作者利用MNIST数据集构建了MNIST-Bag进行可视化实验，详细设置不赘述了，感兴趣可以直接读读原文。总之，作者利用可视化实验讨论了某个学术界小有争议的话题：注意力机制是否具备可解释性。而在本文方法中，答案是肯定的。注意力机制确实发现了包中的目标示例(关键示例，实验中设置为数字9)。对于其余实验效果同样出众，在此不赘述了。</p>
<p><img src="/images/6.png" alt="formula"></p>
<p>Fig.3. </p>
<h3 id="04-总结"><a href="#04-总结" class="headerlink" title="04 总结"></a><u><strong>04</strong></u> 总结</h3><p>总的来说这篇文章的思想很简单，方法也不复杂。尤其是对百家争鸣的今天，方法越来越复杂越来越化庞大，用简单的方法做复杂的事才是最棒的。本文所用到的注意力策略也是非常的基础，但对于多示例学习领域来说是个新的发展方向。尤其是对注意力机制非常感兴趣的我来说，这篇文章对我影响很大。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol>
<li>多示例学习，周志华.</li>
<li>Introduction to Multiple Instance Learning, Marc-André Carbonneau</li>
<li>Revisiting multiple instance neural networks, Pattern Recognition,2018</li>
</ol>
<h4 id="和伙计们运营的公众号，欢迎关注"><a href="#和伙计们运营的公众号，欢迎关注" class="headerlink" title="和伙计们运营的公众号，欢迎关注"></a>和伙计们运营的公众号，欢迎关注</h4><img src="/images/公众号二维码.jpg" alt="wechat" style="zoom:50%;" />]]></content>
      <categories>
        <category>Multi-instance Learning</category>
      </categories>
      <tags>
        <tag>Multi-instance Learning</tag>
        <tag>Attention Mechanism</tag>
      </tags>
  </entry>
</search>
